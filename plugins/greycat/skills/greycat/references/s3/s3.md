# S3 Object Storage

Amazon S3-compatible object storage integration for GreyCat.

## Overview

The S3 library provides seamless integration with Amazon S3 and S3-compatible object storage services (MinIO, Wasabi, DigitalOcean Spaces, etc.). It enables GreyCat applications to store and retrieve objects, manage buckets, and build scalable cloud storage solutions.

Key features include:
- **S3-compatible API** supporting AWS S3, MinIO, and other providers
- **Object operations** including upload, download, list, and delete
- **Bucket management** for creating and listing buckets
- **Credential authentication** using access/secret key pairs
- **Virtual path support** for organizing objects with directory-like structures

This library is ideal for storing large files, backups, user-generated content, data lakes, or building cloud-native applications with distributed object storage.

## Installation

Add the S3 library to your GreyCat project:

```gcl
@library("s3", "7.7.105-dev")
```

## Quick Start

### Connect and Upload a File

```gcl
var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: "AKIAIOSFODNN7EXAMPLE",
    secret_key: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  }
};

// Upload a file
s3.put_object("my-bucket", "/local/path/document.pdf", "documents/2024/document.pdf");

// Download it back
s3.get_object("my-bucket", "documents/2024/document.pdf", "/local/path/downloaded.pdf");
```

### List Objects in a Bucket

```gcl
var objects = s3.list_objects("my-bucket", null, null, null);

for (obj in objects) {
  print("${obj.key} - ${obj.size} bytes - ${obj.last_modified}");
}
```

## Types

### S3

Main connection type representing an S3 client.

**Fields:**
- `host: String` - S3 server address (e.g., `"s3.amazonaws.com"`, `"localhost:9000"`)
- `region: String` - AWS region (e.g., `"us-east-1"`, `"eu-west-1"`)
- `credentials: S3BasicCredentials` - Authentication credentials
- `force_path_style: bool?` - Use path-style URLs instead of virtual-hosted-style (required for MinIO)

**Methods:**
- `list_objects(bucket, prefix?, start_after?, max_keys?): Array<S3Object>`
- `get_object(bucket, key, filepath)`
- `put_object(bucket, filepath, key)`
- `delete_object(bucket, key)`
- `create_bucket(bucket)`
- `list_buckets(prefix?): Array<S3Bucket>`

**Example:**

```gcl
// AWS S3
var awsS3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-west-2",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

// MinIO (local development)
var minioS3 = S3 {
  host: "localhost:9000",
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: "minioadmin",
    secret_key: "minioadmin"
  },
  force_path_style: true // Required for MinIO
};
```

### S3Object

Represents an object stored in S3.

**Fields:**
- `key: String` - Object key/path (e.g., `"24/07/02/record_1234.txt"`)
- `last_modified: time` - Last modification timestamp
- `size: int` - Object size in bytes
- `etag: String` - Entity tag (MD5 hash of the object)

**Example:**

```gcl
var objects = s3.list_objects("my-bucket", "logs/", null, 10);

for (obj in objects) {
  print("File: ${obj.key}");
  print("  Size: ${obj.size} bytes");
  print("  Modified: ${obj.last_modified}");
  print("  ETag: ${obj.etag}");
}
```

### S3Bucket

Represents a bucket in S3.

**Fields:**
- `name: String` - Bucket name
- `creation_date: time` - When the bucket was created

**Example:**

```gcl
var buckets = s3.list_buckets(null);

for (bucket in buckets) {
  print("Bucket: ${bucket.name} (created ${bucket.creation_date})");
}
```

### S3BasicCredentials

Authentication credentials for S3 access.

**Fields:**
- `access_key: String` - AWS access key ID or equivalent
- `secret_key: String` - AWS secret access key or equivalent

**Example:**

```gcl
var creds = S3BasicCredentials {
  access_key: env("S3_ACCESS_KEY"),
  secret_key: env("S3_SECRET_KEY")
};

var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-east-1",
  credentials: creds
};
```

## Methods

### list_objects()

Lists objects in a bucket with optional filtering and pagination.

**Signature:** `fn list_objects(bucket: String, prefix: String?, start_after: String?, max_keys: int?): Array<S3Object>`

**Parameters:**
- `bucket: String` - Bucket name to list from
- `prefix: String?` - Only return keys beginning with this prefix (null = no filter)
- `start_after: String?` - Start listing after this key (for pagination)
- `max_keys: int?` - Maximum number of keys to return (default: 1000, max: 1000)

**Returns:** Array of `S3Object` instances

**Example:**

```gcl
// List all objects (up to 1000)
var all = s3.list_objects("my-bucket", null, null, null);

// List objects with prefix (folder-like filtering)
var logs = s3.list_objects("my-bucket", "logs/2024/", null, null);

// Paginate through large result sets
var page1 = s3.list_objects("my-bucket", null, null, 100);
var lastKey = page1[page1.size() - 1].key;
var page2 = s3.list_objects("my-bucket", null, lastKey, 100);

// List up to 50 objects starting with "data/"
var data = s3.list_objects("my-bucket", "data/", null, 50);
```

### get_object()

Downloads an object from S3 to a local file.

**Signature:** `fn get_object(bucket: String, key: String, filepath: String)`

**Parameters:**
- `bucket: String` - Source bucket name
- `key: String` - Object key to download
- `filepath: String` - Local destination file path

**Behavior:**
- Downloads the entire object
- Overwrites existing local file
- Creates parent directories if needed
- Throws error if object doesn't exist

**Example:**

```gcl
// Download a file
s3.get_object("my-bucket", "images/photo.jpg", "/tmp/photo.jpg");

// Download from virtual directory
s3.get_object("backups", "2024/01/15/database.sql.gz", "/restore/database.sql.gz");

// Download with error handling
try {
  s3.get_object("my-bucket", "important-file.txt", "/tmp/important.txt");
  print("Download successful");
} catch (e) {
  print("Download failed: ${e}");
}
```

### put_object()

Uploads a local file to S3.

**Signature:** `fn put_object(bucket: String, filepath: String, key: String)`

**Parameters:**
- `bucket: String` - Target bucket name
- `filepath: String` - Local source file path
- `key: String` - Object key in S3 (can include virtual directories)

**Behavior:**
- Uploads the entire file
- Overwrites existing S3 object with same key
- Automatically sets content type based on file extension
- Throws error if local file doesn't exist

**Example:**

```gcl
// Upload a file
s3.put_object("my-bucket", "/local/report.pdf", "reports/monthly/jan-2024.pdf");

// Upload to root of bucket
s3.put_object("my-bucket", "/data/config.json", "config.json");

// Upload with timestamp in key
var timestamp = time::now();
s3.put_object("logs", "/var/log/app.log", "logs/${timestamp}/app.log");

// Batch upload
var files = ["file1.txt", "file2.txt", "file3.txt"];
for (file in files) {
  s3.put_object("uploads", "/tmp/${file}", "batch-upload/${file}");
}
```

### delete_object()

Removes an object from S3.

**Signature:** `fn delete_object(bucket: String, key: String)`

**Parameters:**
- `bucket: String` - Bucket containing the object
- `key: String` - Key of the object to delete

**Behavior:**
- Permanently deletes the object
- No error if object doesn't exist (idempotent)
- Cannot be undone (unless versioning is enabled in S3)

**Example:**

```gcl
// Delete a single object
s3.delete_object("my-bucket", "old-file.txt");

// Delete objects by prefix (requires listing first)
var oldLogs = s3.list_objects("logs", "2023/", null, null);
for (obj in oldLogs) {
  s3.delete_object("logs", obj.key);
}

// Cleanup temporary files
s3.delete_object("temp-bucket", "temp/processing-${session_id}.tmp");
```

### create_bucket()

Creates a new S3 bucket.

**Signature:** `fn create_bucket(bucket: String)`

**Parameters:**
- `bucket: String` - Name of the bucket to create

**Behavior:**
- Creates a bucket in the configured region
- Bucket names must be globally unique (AWS) or unique within the service
- Must follow bucket naming rules: lowercase, no underscores, 3-63 characters
- Error if bucket already exists

**Example:**

```gcl
// Create a bucket
s3.create_bucket("my-new-bucket");

// Create with error handling
try {
  s3.create_bucket("analytics-data-2024");
  print("Bucket created");
} catch (e) {
  print("Failed to create bucket: ${e}");
}

// Create multiple buckets for different purposes
var buckets = ["logs", "backups", "uploads", "exports"];
for (name in buckets) {
  try {
    s3.create_bucket("myapp-${name}");
  } catch (e) {
    print("Bucket ${name} already exists or failed: ${e}");
  }
}
```

### list_buckets()

Lists all buckets owned by the authenticated user.

**Signature:** `fn list_buckets(prefix: String?): Array<S3Bucket>`

**Parameters:**
- `prefix: String?` - Only return buckets starting with this prefix (null = all buckets)

**Returns:** Array of `S3Bucket` instances

**Example:**

```gcl
// List all buckets
var buckets = s3.list_buckets(null);
for (bucket in buckets) {
  print("${bucket.name} - created ${bucket.creation_date}");
}

// List buckets with prefix
var appBuckets = s3.list_buckets("myapp-");
print("Found ${appBuckets.size()} application buckets");

// Check if specific bucket exists
var allBuckets = s3.list_buckets(null);
var exists = false;
for (bucket in allBuckets) {
  if (bucket.name == "important-bucket") {
    exists = true;
    break;
  }
}
```

## Common Use Cases

### Backup and Restore

```gcl
var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-west-1",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

// Backup: Upload database dump
var timestamp = time::now();
var backupFile = "/tmp/db-backup-${timestamp}.sql.gz";

// Create backup (external command)
// exec("pg_dump mydb | gzip > ${backupFile}");

// Upload to S3
s3.put_object(
  "database-backups",
  backupFile,
  "backups/${timestamp}/database.sql.gz"
);

print("Backup uploaded to S3");

// Restore: Download specific backup
s3.get_object(
  "database-backups",
  "backups/2024-01-15T10:00:00/database.sql.gz",
  "/tmp/restore.sql.gz"
);

// Restore database (external command)
// exec("gunzip < /tmp/restore.sql.gz | psql mydb");

print("Restore complete");
```

### File Processing Pipeline

```gcl
var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "eu-west-1",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

// List unprocessed files
var files = s3.list_objects("uploads", "incoming/", null, 100);

for (file in files) {
  // Download file
  var localPath = "/tmp/${file.key}";
  s3.get_object("uploads", file.key, localPath);

  // Process file (example: convert, analyze, etc.)
  var processedPath = processFile(localPath);

  // Upload processed result
  s3.put_object("uploads", processedPath, "processed/${file.key}");

  // Move original to archive
  s3.put_object("uploads", localPath, "archive/${file.key}");
  s3.delete_object("uploads", file.key);

  print("Processed: ${file.key}");
}
```

### Multi-Region Replication

```gcl
var sourceS3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

var targetS3 = S3 {
  host: "s3.amazonaws.com",
  region: "eu-west-1",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

// Replicate bucket contents
var objects = sourceS3.list_objects("source-bucket", null, null, null);

for (obj in objects) {
  // Download from source region
  var tmpPath = "/tmp/replication/${obj.key}";
  sourceS3.get_object("source-bucket", obj.key, tmpPath);

  // Upload to target region
  targetS3.put_object("target-bucket", tmpPath, obj.key);

  print("Replicated: ${obj.key}");
}
```

### Static Website Hosting

```gcl
var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: env("AWS_ACCESS_KEY"),
    secret_key: env("AWS_SECRET_KEY")
  }
};

// Upload website files
var websiteFiles = [
  ["index.html", "index.html"],
  ["about.html", "about.html"],
  ["style.css", "css/style.css"],
  ["app.js", "js/app.js"],
  ["logo.png", "images/logo.png"]
];

for (file in websiteFiles) {
  var localPath = "/website/${file[0]}";
  var s3Key = file[1];
  s3.put_object("my-website-bucket", localPath, s3Key);
}

print("Website deployed");
```

## Best Practices

### Credential Management

- **Never hardcode credentials**: Use environment variables or secure configuration
- **Use IAM roles**: In AWS environments, prefer IAM roles over access keys
- **Rotate keys regularly**: Update credentials periodically for security

```gcl
// Good: Load from environment
var s3 = S3 {
  host: env("S3_HOST"),
  region: env("S3_REGION"),
  credentials: S3BasicCredentials {
    access_key: env("S3_ACCESS_KEY"),
    secret_key: env("S3_SECRET_KEY")
  }
};

// Bad: Hardcoded credentials
var s3 = S3 {
  host: "s3.amazonaws.com",
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: "AKIAIOSFODNN7EXAMPLE", // NEVER DO THIS!
    secret_key: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  }
};
```

### Object Naming

- **Use virtual directories**: Organize with `/` separators (e.g., `"2024/01/file.txt"`)
- **Include timestamps**: Makes finding objects easier
- **Avoid special characters**: Stick to alphanumeric, hyphens, underscores, slashes

```gcl
// Good naming conventions
s3.put_object("logs", "/app.log", "logs/2024/01/15/app.log");
s3.put_object("uploads", "/photo.jpg", "users/user-123/photos/2024-01-15-photo.jpg");

// Avoid
s3.put_object("data", "/file.txt", "file with spaces.txt"); // Spaces problematic
s3.put_object("data", "/file.txt", "file@#$.txt"); // Special chars
```

### Performance

- **Batch operations**: Process multiple files in loops
- **Use appropriate max_keys**: Don't fetch more than you need
- **Paginate large listings**: Use `start_after` for buckets with many objects

```gcl
// Efficient pagination
var allObjects = Array<S3Object>{};
var startAfter = null;

while (true) {
  var batch = s3.list_objects("huge-bucket", null, startAfter, 1000);
  allObjects.add_all(batch);

  if (batch.size() < 1000) {
    break; // Last page
  }

  startAfter = batch[batch.size() - 1].key;
}
```

### Error Handling

- **Wrap operations in try-catch**: Network issues, permissions, missing objects
- **Verify uploads**: Check file size or re-download to confirm
- **Handle missing objects gracefully**: `get_object()` fails if key doesn't exist

```gcl
try {
  s3.get_object("my-bucket", "important-file.txt", "/tmp/file.txt");
} catch (e) {
  print("Failed to download: ${e}");
  // Handle missing file or permission error
}
```

### Storage Optimization

- **Delete old objects**: Clean up temporary files regularly
- **Use object lifecycle policies**: Automate archival and deletion (configure in S3)
- **Compress before upload**: Save bandwidth and storage costs

```gcl
// Clean up files older than 30 days
var oldFiles = s3.list_objects("temp-bucket", null, null, null);
var cutoff = time::now() - 30d;

for (file in oldFiles) {
  if (file.last_modified < cutoff) {
    s3.delete_object("temp-bucket", file.key);
    print("Deleted old file: ${file.key}");
  }
}
```

### Gotchas

- **force_path_style**: Required for MinIO and some S3-compatible services
- **Bucket naming**: AWS has strict naming rules (lowercase, no underscores, globally unique)
- **max_keys limit**: Returns at most 1000 objects per call, must paginate for more
- **Object overwrites**: `put_object()` silently replaces existing objects
- **Case sensitivity**: Object keys are case-sensitive (`"file.txt"` != `"File.txt"`)
- **Deletion is permanent**: No trash/recycle bin unless versioning enabled
- **Regional endpoints**: Ensure `host` and `region` match your bucket's location

### Security

- **Use HTTPS**: Production deployments should use encrypted connections
- **Bucket policies**: Configure access policies in S3 console/API
- **Audit access**: Enable logging to track object access

```gcl
// Production configuration
var prodS3 = S3 {
  host: "s3.amazonaws.com", // Uses HTTPS by default
  region: "us-east-1",
  credentials: S3BasicCredentials {
    access_key: env("S3_ACCESS_KEY"),
    secret_key: env("S3_SECRET_KEY")
  }
};
```

### MinIO Configuration

MinIO requires `force_path_style: true`:

```gcl
var minio = S3 {
  host: "localhost:9000",
  region: "us-east-1", // MinIO ignores region but field is required
  credentials: S3BasicCredentials {
    access_key: "minioadmin",
    secret_key: "minioadmin"
  },
  force_path_style: true // REQUIRED for MinIO
};
```
